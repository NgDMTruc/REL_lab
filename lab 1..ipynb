{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after class definition on line 3 (1087732873.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    num_actions, epsilon=ø. 1):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after class definition on line 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedyAgent:\n",
    "\n",
    "    def __init__(self, num_actions, epsilon=0.1):\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.action_values = np.zeros(num_actions)\n",
    "        self.action_counts = np.zeros(num_actions)\n",
    "    # Randomly choose an action for exploration\n",
    "    # Choose the greedy action for exploitation\n",
    "    def select_action(self):\n",
    "        # Your code here\n",
    "\n",
    "        return action\n",
    "    # Update action—value estimate using incremental update rule\n",
    "\n",
    "    def update_value(self, action, reward):\n",
    "    # your code here\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.true_action_values = np.random.normal(0, 1, num_arms)\n",
    "    # Reward is sampled from a normal distribution with mean true action value and unit variance\n",
    "    def get_reward(setf, action):\n",
    "    # your code here\n",
    "num_arms = 10\n",
    "num_steps = 1000\n",
    "agent = EpsilonGreedyAgent(num_arms)\n",
    "\n",
    "bandit = MultiArmedBandit(num_arms)\n",
    "total_rewards = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    action = agent.select_action()\n",
    "    reward = bandit.get_reward(action)\n",
    "    agent.update_value(action, reward)\n",
    "    total_rewards += reward\n",
    "print (\"Total rewards obtained:\" , total_rewards)\n",
    "print (\"Estimated action values:\", agent.action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards obtained: 440.05801874413316\n",
      "Estimated action values: [-0.54186874  0.14697717 -0.43735594 -1.18234497 -0.83651343  0.51686067\n",
      "  0.55289309 -0.95752384 -0.05279096 -0.66404973]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedyAgent:\n",
    "    def __init__(self, num_actions, epsilon=0.1):\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.action_values = np.zeros(num_actions)\n",
    "        self.action_counts = np.zeros(num_actions)\n",
    "\n",
    "    def select_action(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            action = np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest estimated value\n",
    "            action = np.argmax(self.action_values)\n",
    "        return action\n",
    "\n",
    "    def update_value(self, action, reward):\n",
    "        # Incremental update of action value using the sample average method\n",
    "        self.action_counts[action] += 1\n",
    "        alpha = 1 / self.action_counts[action]\n",
    "        self.action_values[action] += alpha * (reward - self.action_values[action])\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.true_action_values = np.random.normal(0, 1, num_arms)\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        # Reward is sampled from a normal distribution with mean true action value and unit variance\n",
    "        reward = np.random.normal(self.true_action_values[action], 1)\n",
    "        return reward\n",
    "\n",
    "# Parameters\n",
    "num_arms = 10\n",
    "num_steps = 1000\n",
    "\n",
    "# Instantiate agent and bandit\n",
    "agent = EpsilonGreedyAgent(num_arms)\n",
    "bandit = MultiArmedBandit(num_arms)\n",
    "\n",
    "# Simulation\n",
    "total_rewards = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    action = agent.select_action()\n",
    "    reward = bandit.get_reward(action)\n",
    "    agent.update_value(action, reward)\n",
    "    total_rewards += reward\n",
    "\n",
    "print(\"Total rewards obtained:\", total_rewards)\n",
    "print(\"Estimated action values:\", agent.action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value function:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grid_size = (3, 3)\n",
    "        # There are Up, Down, Left, Right actions\n",
    "        self.num_actions = 4  # Define number of actions\n",
    "        self.rewards = np.array([\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 1, 0]  # Reward of +1 in the bottom-right cell\n",
    "        ])\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        # Extract the row and column from the state tuple\n",
    "        row, col = state\n",
    "        # Return the reward associated with the given state\n",
    "        return self.rewards[row, col]\n",
    "\n",
    "class ValueFunction:\n",
    "\n",
    "    # Create values array with same size as grid and initialize all elements to zero\n",
    "    def __init__(self, grid_size):\n",
    "        self.values = np.zeros(grid_size)\n",
    "\n",
    "    # Update value at specified state with new_value\n",
    "    def update_value(self, state, new_value):\n",
    "        row, col = state\n",
    "        self.values[row, col] = new_value\n",
    "\n",
    "    # Get value at specified state\n",
    "    def get_value(self, state):\n",
    "        row, col = state\n",
    "        return self.values[row, col]\n",
    "    \n",
    "grid_world = GridWorld()\n",
    "value_function = ValueFunction(grid_world.grid_size)\n",
    "\n",
    "# Initialize the value function with the rewards associated with each cell\n",
    "for i in range(grid_world.grid_size[0]):\n",
    "    for j in range(grid_world.grid_size[1]):\n",
    "        value_function.update_value((i, j), grid_world.get_reward((i, j)))\n",
    "\n",
    "# Print the initial value function to see the value estimates for each state\n",
    "print(\"Initial value function:\")\n",
    "print(value_function.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
